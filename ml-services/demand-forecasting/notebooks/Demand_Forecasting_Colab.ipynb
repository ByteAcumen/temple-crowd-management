{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "intro_text"
   },
   "source": [
    "# ğŸ›ï¸ Temple Crowd Detection â€” YOLOv8 Training Pipeline (Google Colab)\n",
    "\n",
    "**Full end-to-end pipeline: dataset â†’ training â†’ fine-tuning â†’ evaluation â†’ export â†’ download**\n",
    "\n",
    "| Step | What happens |\n",
    "|------|-------------|\n",
    "| 1 | Install packages & verify GPU (T4 / A100) |\n",
    "| 2 | Download Roboflow dataset (Version 5 â€” largest & best labelled) |\n",
    "| 3 | Dataset health check â€” class distribution, image count, label visualisation |\n",
    "| 4 | Train **YOLOv8m** with advanced fine-tuning hyperparameters on GPU |\n",
    "| 5 | Full evaluation â€” mAP@50, mAP@50-95, Precision, Recall, F1 |\n",
    "| 6 | Visual inspection â€” Confusion Matrix, PR Curve, F1 Curve, training loss graphs |\n",
    "| 7 | Real-life inference testing on unseen test images with bounding boxes |\n",
    "| 8 | Export to **ONNX** (production) + **TorchScript** (edge devices) |\n",
    "| 9 | Download `best.pt`, `best.onnx`, all evaluation plots to your PC |\n",
    "\n",
    "---\n",
    "> âœ… **Requirements:** Google Colab with **T4 GPU** enabled  \n",
    "> `Runtime` â†’ `Change runtime type` â†’ `T4 GPU` â†’ `Save`  \n",
    "> Then: `Runtime` â†’ `Run all`  \n",
    ">  \n",
    "> â±ï¸ **Estimated training time:** 20â€“40 min depending on dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 1: Install packages & verify GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!pip install -q ultralytics roboflow\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os, glob, shutil\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"  ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  PyTorch version   : {torch.__version__}\")\n",
    "print(f\"  CUDA available    : {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb  = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"  GPU               : {gpu_name}\")\n",
    "    print(f\"  VRAM              : {vram_gb:.1f} GB\")\n",
    "    BATCH_SIZE = 16 if vram_gb >= 14 else 8   # auto-adjust for smaller GPUs\n",
    "    print(f\"  Auto batch size   : {BATCH_SIZE}\")\n",
    "else:\n",
    "    print(\"  âŒ No GPU detected â€” go to Runtime > Change runtime type > T4 GPU\")\n",
    "    BATCH_SIZE = 4\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"âœ… Setup complete â€” proceed to Cell 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "generate_mock_data"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 2: Download Roboflow Dataset (Version 5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from roboflow import Roboflow\n",
    "\n",
    "rf      = Roboflow(api_key=\"wsdLXeUN3MmbPrvRMrqF\")\n",
    "project = rf.workspace(\"ricky-sambora\").project(\"crowd-detection-7suou\")\n",
    "version = project.version(5)                         # Version 5 = largest dataset\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "# Store dataset root for all future cells\n",
    "DATASET_DIR  = Path(dataset.location)\n",
    "DATA_YAML    = DATASET_DIR / \"data.yaml\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"  DATASET INFO\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Location  : {DATASET_DIR}\")\n",
    "print(f\"  YAML file : {DATA_YAML}\")\n",
    "\n",
    "# Count images per split\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    img_dir = DATASET_DIR / split / \"images\"\n",
    "    lbl_dir = DATASET_DIR / split / \"labels\"\n",
    "    imgs = list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.png\")) if img_dir.exists() else []\n",
    "    lbls = list(lbl_dir.glob(\"*.txt\")) if lbl_dir.exists() else []\n",
    "    print(f\"  {split:<7}: {len(imgs):>4} images  |  {len(lbls):>4} labels\")\n",
    "\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Print class names from data.yaml\n",
    "import yaml\n",
    "with open(DATA_YAML) as f:\n",
    "    meta = yaml.safe_load(f)\n",
    "print(f\"\\n  Classes ({meta.get('nc', '?')}): {meta.get('names', [])}\")\n",
    "print(\"\\nâœ… Dataset ready â€” proceed to Cell 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 3: Dataset Health Check & Label Visualisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.patches as patches\n",
    "import random, cv2\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# â”€â”€ 3a. Class distribution across training labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "label_files = list((DATASET_DIR / \"train\" / \"labels\").glob(\"*.txt\"))\n",
    "class_counts = {}\n",
    "for lf in label_files:\n",
    "    with open(lf) as f:\n",
    "        for line in f:\n",
    "            cls = int(line.split()[0])\n",
    "            class_counts[cls] = class_counts.get(cls, 0) + 1\n",
    "\n",
    "class_names = meta.get('names', [str(i) for i in range(meta.get('nc', 1))])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "fig.suptitle('Dataset Health Check', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart â€” class distribution\n",
    "labels  = [class_names[k] for k in sorted(class_counts)]\n",
    "counts  = [class_counts[k] for k in sorted(class_counts)]\n",
    "axes[0].bar(labels, counts, color='steelblue')\n",
    "axes[0].set_title('Object Count per Class (Train split)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=30)\n",
    "for i, v in enumerate(counts):\n",
    "    axes[0].text(i, v + 5, str(v), ha='center', fontsize=9)\n",
    "\n",
    "# Image count per split\n",
    "splits = ['train', 'valid', 'test']\n",
    "split_counts = []\n",
    "for s in splits:\n",
    "    d = DATASET_DIR / s / \"images\"\n",
    "    split_counts.append(len(list(d.glob(\"*.jpg\")) + list(d.glob(\"*.png\"))) if d.exists() else 0)\n",
    "axes[1].bar(splits, split_counts, color=['steelblue', 'coral', 'mediumseagreen'])\n",
    "axes[1].set_title('Images per Split')\n",
    "axes[1].set_ylabel('Image Count')\n",
    "for i, v in enumerate(split_counts):\n",
    "    axes[1].text(i, v + 2, str(v), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dataset_health.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€ 3b. Sample annotated images from training set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_imgs = list((DATASET_DIR / \"train\" / \"images\").glob(\"*.jpg\"))\n",
    "samples    = random.sample(train_imgs, min(6, len(train_imgs)))\n",
    "\n",
    "fig2, ax2 = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig2.suptitle('Sample Training Images with Ground-Truth Annotations', fontsize=13)\n",
    "for ax, img_path in zip(ax2.flat, samples):\n",
    "    img = plt.imread(str(img_path))\n",
    "    h, w = img.shape[:2]\n",
    "    ax.imshow(img)\n",
    "    lbl_path = img_path.parent.parent / \"labels\" / (img_path.stem + \".txt\")\n",
    "    if lbl_path.exists():\n",
    "        with open(lbl_path) as f:\n",
    "            for line in f:\n",
    "                parts = list(map(float, line.split()))\n",
    "                cx, cy, bw, bh = parts[1]*w, parts[2]*h, parts[3]*w, parts[4]*h\n",
    "                x1, y1 = cx - bw/2, cy - bh/2\n",
    "                rect = patches.Rectangle((x1,y1), bw, bh, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(img_path.name[:20], fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_annotations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Health check done â€” dataset_health.png & sample_annotations.png saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 4: Train YOLOv8m with Advanced Fine-Tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# YOLOv8m = Medium variant â€” best accuracy/speed balance for crowd detection\n",
    "# All hyperparameters are tuned specifically for dense crowd/people detection\n",
    "\n",
    "model = YOLO('yolov8m.pt')   # starts from COCO-pretrained weights (transfer learning)\n",
    "\n",
    "print(\"ğŸš€ Starting training on GPU...\")\n",
    "print(f\"   Model     : YOLOv8m (transfer learning from COCO)\")\n",
    "print(f\"   Dataset   : {DATA_YAML}\")\n",
    "print(f\"   Batch     : {BATCH_SIZE}\")\n",
    "print()\n",
    "\n",
    "results = model.train(\n",
    "    # â”€â”€ Core settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    data     = str(DATA_YAML),\n",
    "    epochs   = 100,           # max epochs; early stopping will cut short if needed\n",
    "    imgsz    = 640,           # standard YOLO input resolution\n",
    "    batch    = BATCH_SIZE,    # auto-set to 16 (T4) or 8 (smaller GPUs)\n",
    "    device   = 0,             # use GPU 0 (T4 on Colab)\n",
    "    workers  = 2,             # data-loading threads\n",
    "\n",
    "    # â”€â”€ Regularisation & overfitting prevention â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    dropout  = 0.1,           # 10% dropout â€” prevents memorising one temple\n",
    "    patience = 20,            # early stopping: halt if no gain for 20 epochs\n",
    "    weight_decay = 0.0005,    # L2 regularisation\n",
    "\n",
    "    # â”€â”€ Learning rate scheduling (cosine) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    optimizer = 'AdamW',      # AdamW = best for fine-tuning pretrained models\n",
    "    cos_lr    = True,         # cosine annealing â€” smooth LR decay to minimum\n",
    "    lr0       = 0.001,        # initial LR (lower than default for fine-tuning)\n",
    "    lrf       = 0.1,          # final LR = lr0 Ã— lrf = 0.0001\n",
    "\n",
    "    # â”€â”€ Training augmentations (prevent overfitting on small datasets) â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    hsv_h    = 0.015,         # hue jitter (lighting variations)\n",
    "    hsv_s    = 0.7,           # saturation jitter (outdoor/indoor scenes)\n",
    "    hsv_v    = 0.4,           # brightness jitter (day/night/flash)\n",
    "    fliplr   = 0.5,           # 50% horizontal flip\n",
    "    mosaic   = 1.0,           # mosaic augmentation â€” combine 4 images\n",
    "    mixup    = 0.1,           # MixUp blending â€” helps generalise\n",
    "    copy_paste = 0.1,         # copy-paste crowd instances across images\n",
    "\n",
    "    # â”€â”€ Output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    plots    = True,          # auto-generate all evaluation charts\n",
    "    save     = True,\n",
    "    project  = 'runs/detect',\n",
    "    name     = 'crowd_train',\n",
    "    exist_ok = False,         # create a fresh numbered dir if run again\n",
    ")\n",
    "\n",
    "# Capture the exact save directory â€” NEVER hardcode 'runs/detect/crowd_train'\n",
    "# because Ultralytics auto-increments (crowd_train2, crowd_train3 ...) on re-runs\n",
    "TRAIN_DIR   = Path(results.save_dir)\n",
    "WEIGHTS_DIR = TRAIN_DIR / 'weights'\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"ğŸ“ Results saved to : {TRAIN_DIR}\")\n",
    "print(f\"ğŸ† Best weights     : {WEIGHTS_DIR / 'best.pt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "train_prophet"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 5: Full Model Evaluation (Validation Split) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from ultralytics import YOLO\n",
    "\n",
    "best_pt    = WEIGHTS_DIR / 'best.pt'\n",
    "best_model = YOLO(str(best_pt))\n",
    "\n",
    "print(f\"âœ… Loaded: {best_pt}\")\n",
    "print()\n",
    "best_model.info()   # print layer count, parameters, FLOPS\n",
    "\n",
    "print(\"\\nâ³ Running validation on held-out validation split...\")\n",
    "metrics = best_model.val(\n",
    "    data   = str(DATA_YAML),\n",
    "    imgsz  = 640,\n",
    "    device = 0,\n",
    "    plots  = True,   # saves confusion_matrix.png, PR_curve.png, F1_curve.png\n",
    "    save_json = True # saves predictions in COCO JSON format\n",
    ")\n",
    "\n",
    "# â”€â”€ Per-class results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"  VALIDATION METRICS â€” per class\")\n",
    "print(\"=\" * 55)\n",
    "names = best_model.names\n",
    "for i, name in names.items():\n",
    "    try:\n",
    "        ap50    = metrics.box.ap50[i]\n",
    "        ap      = metrics.box.ap[i]\n",
    "        print(f\"  {name:<20}  mAP@50={ap50:.4f}  mAP@50-95={ap:.4f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print()\n",
    "print(\"=\" * 55)\n",
    "print(\"  OVERALL METRICS\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  mAP@50       : {metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP@50-95    : {metrics.box.map:.4f}   â† PRIMARY metric\")\n",
    "print(f\"  mAP@75       : {metrics.box.map75:.4f}\")\n",
    "print(f\"  Precision    : {metrics.box.mp:.4f}\")\n",
    "print(f\"  Recall       : {metrics.box.mr:.4f}\")\n",
    "f1 = 2 * metrics.box.mp * metrics.box.mr / (metrics.box.mp + metrics.box.mr + 1e-9)\n",
    "print(f\"  F1 Score     : {f1:.4f}\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Quality gate\n",
    "if metrics.box.map50 >= 0.80:\n",
    "    print(\"ğŸŸ¢ EXCELLENT â€” mAP@50 â‰¥ 80%  (production ready)\")\n",
    "elif metrics.box.map50 >= 0.65:\n",
    "    print(\"ğŸŸ¡ GOOD      â€” mAP@50 â‰¥ 65%  (usable, more data helps)\")\n",
    "else:\n",
    "    print(\"ğŸ”´ LOW       â€” mAP@50 < 65%  (consider more epochs / data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 6: Visual Evaluation Plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Display all auto-generated training and validation charts\n",
    "\n",
    "plots_to_display = {\n",
    "    'Training Loss & mAP Curves'  : 'results.png',\n",
    "    'Confusion Matrix (normalised)': 'confusion_matrix_normalized.png',\n",
    "    'Confusion Matrix (raw)'       : 'confusion_matrix.png',\n",
    "    'Precision-Recall Curve'       : 'PR_curve.png',\n",
    "    'F1-Confidence Curve'          : 'F1_curve.png',\n",
    "    'Precision-Confidence Curve'   : 'P_curve.png',\n",
    "    'Recall-Confidence Curve'      : 'R_curve.png',\n",
    "    'Sample Training Batch'        : 'train_batch0.jpg',\n",
    "    'Sample Validation Batch'      : 'val_batch0_pred.jpg',\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Loading evaluation charts from:\", TRAIN_DIR)\n",
    "shown = 0\n",
    "for title, fname in plots_to_display.items():\n",
    "    fpath = TRAIN_DIR / fname\n",
    "    if fpath.exists():\n",
    "        print(f\"\\n{'â”€'*50}\")\n",
    "        print(f\"  {title}\")\n",
    "        print(f\"{'â”€'*50}\")\n",
    "        display(Image(filename=str(fpath), width=800))\n",
    "        shown += 1\n",
    "    else:\n",
    "        # try the val/ subdirectory where ultralytics saves some validation plots\n",
    "        alt = TRAIN_DIR / 'val' / fname\n",
    "        if alt.exists():\n",
    "            print(f\"\\n  {title}\")\n",
    "            display(Image(filename=str(alt), width=800))\n",
    "            shown += 1\n",
    "\n",
    "print(f\"\\nâœ… Displayed {shown} / {len(plots_to_display)} evaluation plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 7: Real-Life Inference Testing on Unseen Images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import random\n",
    "\n",
    "# Source: test split â†’ fallback to valid split if test doesn't exist\n",
    "test_img_dir  = DATASET_DIR / 'test'  / 'images'\n",
    "valid_img_dir = DATASET_DIR / 'valid' / 'images'\n",
    "\n",
    "if test_img_dir.exists() and len(list(test_img_dir.glob('*.jpg'))) > 0:\n",
    "    img_source = test_img_dir\n",
    "    split_used = 'test'\n",
    "else:\n",
    "    img_source = valid_img_dir\n",
    "    split_used = 'valid'\n",
    "    print(\"âš ï¸  No test split found â€” using validation images for visual check\")\n",
    "\n",
    "all_imgs = list(img_source.glob('*.jpg')) + list(img_source.glob('*.png'))\n",
    "sample   = random.sample(all_imgs, min(8, len(all_imgs)))   # pick 8 random images\n",
    "\n",
    "print(f\"ğŸ” Running inference on {len(sample)} unseen {split_used} images\")\n",
    "print(f\"   Confidence threshold : 0.30\")\n",
    "print(f\"   IoU threshold (NMS)  : 0.50\\n\")\n",
    "\n",
    "preds = best_model.predict(\n",
    "    source  = [str(p) for p in sample],\n",
    "    imgsz   = 640,\n",
    "    conf    = 0.30,       # 30% confidence â€” good balance; lower = more detections\n",
    "    iou     = 0.50,       # NMS IoU threshold\n",
    "    save    = True,       # save annotated images to disk\n",
    "    verbose = False,\n",
    ")\n",
    "\n",
    "# Show annotated predictions\n",
    "print(\"ğŸ“¸ Annotated predictions:\\n\")\n",
    "for r in preds:\n",
    "    out_path = Path(r.save_dir) / Path(r.path).name\n",
    "    if not out_path.exists():\n",
    "        # try globbing for the saved file\n",
    "        matches = list(Path(r.save_dir).glob(Path(r.path).stem + '*'))\n",
    "        out_path = matches[0] if matches else None\n",
    "    n_boxes = len(r.boxes)\n",
    "    crowd_density = \"ğŸŸ¢ Low\" if n_boxes < 5 else (\"ğŸŸ¡ Medium\" if n_boxes < 15 else \"ğŸ”´ High\")\n",
    "    print(f\"  {Path(r.path).name:<35}  Detections: {n_boxes:>3}  Crowd: {crowd_density}\")\n",
    "    if out_path and out_path.exists():\n",
    "        display(Image(filename=str(out_path), width=600))\n",
    "\n",
    "# â”€â”€ Crowd density summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_counts = [len(r.boxes) for r in preds]\n",
    "print(f\"\\nğŸ“Š Inference Summary:\")\n",
    "print(f\"  Images tested        : {len(preds)}\")\n",
    "print(f\"  Avg detections/image : {np.mean(all_counts):.1f}\")\n",
    "print(f\"  Max detections       : {max(all_counts)}\")\n",
    "print(f\"  Min detections       : {min(all_counts)}\")\n",
    "print()\n",
    "print(\"ğŸ¥ To test on a VIDEO, upload your .mp4 to Colab then run:\")\n",
    "print(\"   best_model.predict('your_video.mp4', save=True, conf=0.30, stream=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "make_predictions"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 8: Export to Production Formats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“¦ Exporting trained model for production deployment...\\n\")\n",
    "\n",
    "# â”€â”€ ONNX export (for Python/Node.js backend â€” no PyTorch needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"1ï¸âƒ£  Exporting to ONNX...\")\n",
    "onnx_path = best_model.export(\n",
    "    format   = 'onnx',\n",
    "    imgsz    = 640,\n",
    "    dynamic  = True,      # variable batch size â€” required for live camera streams\n",
    "    simplify = True,      # remove redundant ONNX nodes â€” smaller & faster\n",
    "    opset    = 17,        # ONNX opset 17 = widest tool compatibility\n",
    ")\n",
    "ONNX_PATH = Path(str(onnx_path))\n",
    "print(f\"   âœ… ONNX saved  : {ONNX_PATH}\")\n",
    "\n",
    "# â”€â”€ TorchScript export (for edge deployment â†’ Raspberry Pi, Jetson Nano) â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n2ï¸âƒ£  Exporting to TorchScript...\")\n",
    "try:\n",
    "    ts_path = best_model.export(format='torchscript', imgsz=640, optimize=True)\n",
    "    TS_PATH = Path(str(ts_path))\n",
    "    print(f\"   âœ… TorchScript : {TS_PATH}\")\n",
    "except Exception as e:\n",
    "    TS_PATH = None\n",
    "    print(f\"   âš ï¸  TorchScript skipped: {e}\")\n",
    "\n",
    "# â”€â”€ File size summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"  EXPORT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "export_files = [\n",
    "    (WEIGHTS_DIR / 'best.pt',  'PyTorch weights (primary)'),\n",
    "    (WEIGHTS_DIR / 'last.pt',  'Last epoch weights (backup)'),\n",
    "    (ONNX_PATH,                'ONNX (backend deployment)'),\n",
    "]\n",
    "if TS_PATH:\n",
    "    export_files.append((TS_PATH, 'TorchScript (edge devices)'))\n",
    "\n",
    "for fpath, desc in export_files:\n",
    "    if fpath and Path(fpath).exists():\n",
    "        size_mb = Path(fpath).stat().st_size / 1e6\n",
    "        print(f\"  âœ… {size_mb:>6.1f} MB  {desc}\")\n",
    "        print(f\"           {fpath}\")\n",
    "    else:\n",
    "        print(f\"  âŒ Not found : {desc}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 9: Full Training Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 60)\n",
    "print(\"  ğŸ›ï¸  TEMPLE CROWD DETECTION â€” TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model performance\n",
    "print(f\"\\n  ğŸ“Š Model Performance:\")\n",
    "print(f\"     mAP@50       : {metrics.box.map50:.4f}\")\n",
    "print(f\"     mAP@50-95    : {metrics.box.map:.4f}\")\n",
    "print(f\"     Precision    : {metrics.box.mp:.4f}\")\n",
    "print(f\"     Recall       : {metrics.box.mr:.4f}\")\n",
    "f1 = 2 * metrics.box.mp * metrics.box.mr / (metrics.box.mp + metrics.box.mr + 1e-9)\n",
    "print(f\"     F1 Score     : {f1:.4f}\")\n",
    "\n",
    "# Training config\n",
    "print(f\"\\n  âš™ï¸  Training Config:\")\n",
    "print(f\"     Base model   : YOLOv8m (COCO pretrained)\")\n",
    "print(f\"     Dataset      : Roboflow crowd-detection v5\")\n",
    "print(f\"     Batch size   : {BATCH_SIZE}\")\n",
    "print(f\"     Image size   : 640Ã—640\")\n",
    "print(f\"     Optimizer    : AdamW + Cosine LR\")\n",
    "print(f\"     Dropout      : 10%\")\n",
    "print(f\"     Augmentations: Mosaic, MixUp, HSV, Flip, CopyPaste\")\n",
    "\n",
    "# Saved files\n",
    "print(f\"\\n  ğŸ“ Saved Files:\")\n",
    "all_save = [\n",
    "    (WEIGHTS_DIR / 'best.pt',          'best.pt      â€” best checkpoint'),\n",
    "    (WEIGHTS_DIR / 'last.pt',          'last.pt      â€” final epoch'),\n",
    "    (ONNX_PATH,                        'best.onnx    â€” ONNX production'),\n",
    "    (TRAIN_DIR / 'results.png',        'results.png  â€” loss/mAP curves'),\n",
    "    (TRAIN_DIR / 'confusion_matrix_normalized.png', 'confusion_matrix_normalized.png'),\n",
    "    (TRAIN_DIR / 'PR_curve.png',       'PR_curve.png'),\n",
    "    (TRAIN_DIR / 'F1_curve.png',       'F1_curve.png'),\n",
    "    ('dataset_health.png',             'dataset_health.png'),\n",
    "    ('sample_annotations.png',         'sample_annotations.png'),\n",
    "]\n",
    "for fpath, label in all_save:\n",
    "    exists = Path(str(fpath)).exists()\n",
    "    mark   = 'âœ…' if exists else 'âŒ'\n",
    "    size   = f\"{Path(str(fpath)).stat().st_size/1e6:.1f} MB\" if exists else \"missing\"\n",
    "    print(f\"     {mark} {label:<45} {size}\")\n",
    "\n",
    "print(f\"\\n  ğŸ“‚ Full train dir: {TRAIN_DIR}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 10: Download All Trained Models & Plots to Your PC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Each files.download() triggers a browser Save-As dialog (or saves to the\n",
    "# VS Code Colab file explorer panel on the left).\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "download_targets = [\n",
    "    # Models (most important â€” copy these into your project)\n",
    "    (str(WEIGHTS_DIR / 'best.pt'),   'ğŸ† best.pt         PyTorch weights (primary model)'),\n",
    "    (str(ONNX_PATH),                 'âš¡ best.onnx        ONNX (Node.js/Python backend)'),\n",
    "    (str(WEIGHTS_DIR / 'last.pt'),   'ğŸ’¾ last.pt         Last epoch (backup)'),\n",
    "\n",
    "    # Evaluation plots\n",
    "    (str(TRAIN_DIR / 'results.png'),              'ğŸ“ˆ results.png              Training curves'),\n",
    "    (str(TRAIN_DIR / 'confusion_matrix_normalized.png'), 'ğŸ”¢ confusion_matrix_norm.png'),\n",
    "    (str(TRAIN_DIR / 'PR_curve.png'),             'ğŸ“‰ PR_curve.png             Precision-Recall'),\n",
    "    (str(TRAIN_DIR / 'F1_curve.png'),             'ğŸ“Š F1_curve.png             F1 vs Confidence'),\n",
    "    (str(TRAIN_DIR / 'val_batch0_pred.jpg'),      'ğŸ–¼ï¸  val_batch0_pred.jpg      Val predictions'),\n",
    "\n",
    "    # Dataset analysis\n",
    "    ('dataset_health.png',                        'ğŸ“‹ dataset_health.png       Class distribution'),\n",
    "    ('sample_annotations.png',                    'ğŸ” sample_annotations.png   GT annotations'),\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¥ Starting downloads...\\n\")\n",
    "success, skipped = 0, 0\n",
    "for fpath, description in download_targets:\n",
    "    if os.path.exists(fpath):\n",
    "        files.download(fpath)\n",
    "        print(f\"  âœ… {description}\")\n",
    "        success += 1\n",
    "    else:\n",
    "        print(f\"  âš ï¸  Skipped (not found): {os.path.basename(fpath)}\")\n",
    "        skipped += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  Downloaded : {success} files\")\n",
    "print(f\"  Skipped    : {skipped} files (run missing cells first)\")\n",
    "print(f\"{'='*60}\")\n",
    "print()\n",
    "print(\"ğŸ“ WHERE TO PUT THE FILES IN YOUR PROJECT:\")\n",
    "print()\n",
    "print(\"  best.pt   â†’ ml-services/crowd-detection/models/best.pt\")\n",
    "print(\"  best.onnx â†’ ml-services/crowd-detection/models/best.onnx\")\n",
    "print()\n",
    "print(\"ğŸ”— To use best.onnx in your Node.js backend:\")\n",
    "print(\"   const { InferenceSession } = require('onnxruntime-node')\")\n",
    "print(\"   const session = await InferenceSession.create('./models/best.onnx')\")\n",
    "print()\n",
    "print(\"ğŸ‰ All done! Your crowd detection model is ready for production.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
