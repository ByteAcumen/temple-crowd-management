{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro"
            },
            "source": [
                "# ğŸš€ Ultimate Temple Crowd Detection - YOLOv8 Pro Pipeline\n",
                "\n",
                "**Features Included:**\n",
                "- Uses Roboflow **Version 5** (Best & largest dataset)\n",
                "- Trains **YOLOv8m** (Medium) for maximum accuracy on crowds\n",
                "- **Advanced Fine-Tuning** (CosLR, Augmentations, Early Stopping)\n",
                "- **Pro Evaluation** (Confusion Matrix, mAP Metrics)\n",
                "- **Real-Life Testing** (Image & Video Inference)\n",
                "- **Export & Download** (PyTorch `.pt` and Production `.onnx` formats)\n",
                "\n",
                "**Instructions:**\n",
                "Ensure you are running this in Google Colab with a **T4 GPU** enabled (`Runtime` > `Change runtime type` > `T4 GPU`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "# 1. Setup Environment & Check GPU\n",
                "!pip install -q ultralytics roboflow\n",
                "\n",
                "import torch\n",
                "from ultralytics import YOLO\n",
                "\n",
                "print(\"âœ… Setup Complete!\")\n",
                "print(f\"ğŸ”¥ GPU Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"ğŸ’» GPU Device: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download"
            },
            "outputs": [],
            "source": [
                "# 2. Download Best Dataset (Version 5) from Roboflow\n",
                "from roboflow import Roboflow\n",
                "\n",
                "rf = Roboflow(api_key=\"wsdLXeUN3MmbPrvRMrqF\") # Your API Key\n",
                "project = rf.workspace(\"ricky-sambora\").project(\"crowd-detection-7suou\")\n",
                "\n",
                "# Using Version 5 (The larger, better dataset)\n",
                "version = project.version(5)\n",
                "dataset = version.download(\"yolov8\")\n",
                "\n",
                "print(f\"âœ… Dataset Version 5 downloaded to: {dataset.location}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train"
            },
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 3: Advanced Model Training & Fine-Tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "from ultralytics import YOLO\n",
                "import os\n",
                "\n",
                "# Initialize YOLOv8 Medium â€” best balance of speed and accuracy for crowd detection\n",
                "model = YOLO('yolov8m.pt')\n",
                "\n",
                "print(\"ğŸš€ Starting advanced training pipeline on T4 GPU...\")\n",
                "results = model.train(\n",
                "    data=f\"{dataset.location}/data.yaml\",\n",
                "    epochs=100,           # Train up to 100 epochs (early stopping will cut short if needed)\n",
                "    imgsz=640,            # Standard resolution â€” best balance for detection\n",
                "    batch=16,             # Optimal for T4 (16GB VRAM); reduce to 8 if OOM error\n",
                "    patience=25,          # Stop early if no improvement for 25 epochs\n",
                "    device=0,             # Force GPU usage\n",
                "    optimizer='auto',     # Ultralytics auto-selects AdamW or SGD\n",
                "    cos_lr=True,          # Cosine LR schedule â€” smoother convergence than step decay\n",
                "    lr0=0.01,             # Initial learning rate\n",
                "    lrf=0.1,              # Final LR = lr0 * lrf = 0.001 (sensible decay)\n",
                "    dropout=0.1,          # Light dropout to reduce overfitting\n",
                "    plots=True,           # Auto-generate confusion matrix, F1 curve, PR curve, results.png\n",
                ")\n",
                "\n",
                "# âš ï¸ IMPORTANT: Save the exact run directory â€” YOLO auto-increments (train, train2, train3...)\n",
                "# Hardcoding 'runs/detect/train' WILL break if you run training more than once.\n",
                "TRAIN_DIR = str(results.save_dir)\n",
                "WEIGHTS_DIR = os.path.join(TRAIN_DIR, 'weights')\n",
                "\n",
                "print(f\"\\nâœ… Training Complete!\")\n",
                "print(f\"ğŸ“ Train run saved to : {TRAIN_DIR}\")\n",
                "print(f\"ğŸ† Best weights at   : {WEIGHTS_DIR}/best.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "evaluate"
            },
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 4: Detailed Model Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "from IPython.display import display, Image\n",
                "from ultralytics import YOLO\n",
                "import glob, os\n",
                "\n",
                "# Load the BEST weights from this specific training run (uses dynamic path â€” never breaks)\n",
                "best_pt = os.path.join(WEIGHTS_DIR, 'best.pt')\n",
                "best_model = YOLO(best_pt)\n",
                "\n",
                "print(f\"âœ… Loaded best weights: {best_pt}\")\n",
                "print(f\"\\nğŸ“‹ Model summary:\")\n",
                "best_model.info()\n",
                "\n",
                "# --- Validate on the validation split ---\n",
                "metrics = best_model.val()\n",
                "\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"ğŸ“Š  FINAL VALIDATION METRICS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"  mAP@50-95  (primary metric) : {metrics.box.map:.4f}\")\n",
                "print(f\"  mAP@50                      : {metrics.box.map50:.4f}\")\n",
                "print(f\"  mAP@75                      : {metrics.box.map75:.4f}\")\n",
                "print(f\"  Precision                   : {metrics.box.mp:.4f}\")\n",
                "print(f\"  Recall                      : {metrics.box.mr:.4f}\")\n",
                "print(\"=\" * 50)\n",
                "if metrics.box.map50 >= 0.80:\n",
                "    print(\"ğŸŸ¢ Excellent â€” mAP@50 above 80%\")\n",
                "elif metrics.box.map50 >= 0.60:\n",
                "    print(\"ğŸŸ¡ Good â€” mAP@50 above 60%\")\n",
                "else:\n",
                "    print(\"ğŸ”´ Needs improvement â€” consider more data or epochs\")\n",
                "\n",
                "# --- Display training plots (all paths built dynamically from TRAIN_DIR) ---\n",
                "plots_to_show = {\n",
                "    'Training Loss & Validation Curves': 'results.png',\n",
                "    'Confusion Matrix':                  'confusion_matrix.png',\n",
                "    'F1-Confidence Curve':               'F1_curve.png',\n",
                "    'Precision-Recall Curve':            'PR_curve.png',\n",
                "}\n",
                "for title, filename in plots_to_show.items():\n",
                "    path = os.path.join(TRAIN_DIR, filename)\n",
                "    if os.path.exists(path):\n",
                "        print(f\"\\nğŸ“ˆ {title}:\")\n",
                "        display(Image(filename=path, width=800))\n",
                "    else:\n",
                "        print(f\"âš ï¸  {filename} not found â€” skipping\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_real_life"
            },
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 5: Real-Life Testing on Unseen Images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "from IPython.display import display, Image\n",
                "import glob, os\n",
                "\n",
                "print(\"ğŸ” Testing best model on unseen test set images...\")\n",
                "\n",
                "# Try test/images first, fall back to valid/images if test split doesn't exist\n",
                "test_images = glob.glob(f\"{dataset.location}/test/images/*.jpg\")\n",
                "if not test_images:\n",
                "    print(\"âš ï¸  No test/images found â€” falling back to valid/images for visual check\")\n",
                "    test_images = glob.glob(f\"{dataset.location}/valid/images/*.jpg\")\n",
                "\n",
                "if not test_images:\n",
                "    print(\"âŒ No images found for inference. Check dataset download.\")\n",
                "else:\n",
                "    # Use up to 5 test images\n",
                "    sample_images = test_images[:5]\n",
                "    print(f\"ğŸ“¸ Running inference on {len(sample_images)} sample image(s)...\\n\")\n",
                "\n",
                "    # Run prediction (saves annotated images to runs/detect/predictX/)\n",
                "    predictions = best_model.predict(\n",
                "        source=sample_images,\n",
                "        save=True,           # saves annotated image to disk\n",
                "        conf=0.25,           # 25% confidence threshold\n",
                "        verbose=False\n",
                "    )\n",
                "\n",
                "    for r in predictions:\n",
                "        # r.save_dir = the predict output directory (e.g. runs/detect/predict2)\n",
                "        # r.path     = the source image path used for this prediction\n",
                "        saved_img = os.path.join(r.save_dir, os.path.basename(r.path))\n",
                "        if os.path.exists(saved_img):\n",
                "            print(f\"âœ… {os.path.basename(r.path)} â€” {len(r.boxes)} detections\")\n",
                "            display(Image(filename=saved_img, width=500))\n",
                "        else:\n",
                "            # Fallback: glob the predict dir for any result image\n",
                "            fallback = glob.glob(os.path.join(r.save_dir, '*.jpg'))\n",
                "            if fallback:\n",
                "                display(Image(filename=fallback[0], width=500))\n",
                "\n",
                "    print(\"\\nğŸ¥ To test on a VIDEO file:\")\n",
                "    print(\"  1. Upload your .mp4 to Colab (left panel â†’ Upload)\")\n",
                "    print(\"  2. Run: best_model.predict('your_video.mp4', save=True, conf=0.3)\")\n",
                "    print(\"  3. The annotated video will be saved in runs/detect/predictX/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "export"
            },
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 6: Export to Production Formats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import os\n",
                "\n",
                "print(\"ğŸ“¦ Exporting model for production use...\")\n",
                "\n",
                "# Export to ONNX â€” platform-independent, works in Python/Node.js without PyTorch\n",
                "# dynamic=True allows variable batch sizes (required for live camera streams)\n",
                "onnx_export_path = best_model.export(format='onnx', dynamic=True, simplify=True)\n",
                "\n",
                "# Ultralytics returns the ONNX path â€” save it for the download step below\n",
                "ONNX_PATH = str(onnx_export_path)\n",
                "\n",
                "print(f\"\\nâœ… Exports complete!\")\n",
                "print(f\"  PyTorch weights : {os.path.join(WEIGHTS_DIR, 'best.pt')}\")\n",
                "print(f\"  ONNX model      : {ONNX_PATH}\")\n",
                "print(\"\\nğŸ“Œ Use best.onnx in your Node.js/Python backend for fast inference without PyTorch overhead\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download_files"
            },
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 7: Download Trained Models to Your Windows PC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# Each files.download() triggers a browser Save-As dialog (or saves to VS Code file explorer).\n",
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "# Paths built dynamically from TRAIN_DIR captured during training (never hardcoded)\n",
                "PT_PATH   = os.path.join(WEIGHTS_DIR, 'best.pt')    # PyTorch weights\n",
                "LAST_PATH = os.path.join(WEIGHTS_DIR, 'last.pt')    # Last epoch weights (backup)\n",
                "\n",
                "download_targets = [\n",
                "    (PT_PATH,   'ğŸ† best.pt   â€” best checkpoint (use this in production)'),\n",
                "    (ONNX_PATH, 'âš¡ best.onnx â€” production ONNX (Node.js/Python, no PyTorch needed)'),\n",
                "    (LAST_PATH, 'ğŸ’¾ last.pt   â€” final epoch checkpoint (backup)'),\n",
                "]\n",
                "\n",
                "print(\"ğŸ“¥ Starting model downloads...\\n\")\n",
                "for filepath, description in download_targets:\n",
                "    if os.path.exists(filepath):\n",
                "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
                "        files.download(filepath)\n",
                "        print(f\"  âœ… {description}\")\n",
                "        print(f\"     Size: {size_mb:.1f} MB  â†’  {filepath}\")\n",
                "    else:\n",
                "        print(f\"  âŒ Not found (run previous cells first): {filepath}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 55)\n",
                "print(\"ğŸ‰ DONE! Next steps:\")\n",
                "print(\"=\" * 55)\n",
                "print(\"1. Copy best.pt   â†’ ml-services/crowd-detection/models/best.pt\")\n",
                "print(\"2. Copy best.onnx â†’ ml-services/crowd-detection/models/best.onnx\")\n",
                "print(\"3. Use best.onnx in your backend for live crowd detection\")\n",
                "print(f\"\\nğŸ“Š Training results folder: {TRAIN_DIR}\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
